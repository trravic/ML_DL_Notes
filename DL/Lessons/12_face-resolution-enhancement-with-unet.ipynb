{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":29963,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Super-Resolution using U-Net\n\nThis notebook demonstrates how to build and train a U-Net model for image super-resolution.  \nThe goal is to take a low-resolution image (64x64) and generate a high-resolution version (128x128).\n\nThe U-Net architecture is well-suited for this task as it effectively captures both local and global features through its encoder-decoder structure with skip connections.\n\n![Super Resolution Example](https://raw.githubusercontent.com/AshishJangra27/ai-projects/refs/heads/main/Image%20Enhancement%20with%20U-Net/img.png)\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Setup and Imports\n\nThis cell imports all the necessary libraries for building and training the U-Net model, including Keras for model definition, OpenCV for image processing, and Matplotlib for visualization.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T11:31:16.543172Z","iopub.execute_input":"2025-09-11T11:31:16.543449Z","iopub.status.idle":"2025-09-11T11:31:16.549036Z","shell.execute_reply.started":"2025-09-11T11:31:16.543425Z","shell.execute_reply":"2025-09-11T11:31:16.548158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. U-Net Model Definition\n\nThis cell defines the U-Net model architecture for image super-resolution. The `unet_64to128` function creates a U-Net model that takes a 64x64x3 image as input and outputs a 128x128x3 image. It includes an encoder, a bottleneck, and a decoder with skip connections.","metadata":{}},{"cell_type":"code","source":"def unet_64to128(input_shape=(64, 64, 3), n_classes=3, final_activation='sigmoid', dropout_rate=0.05):\n    inputs = Input(shape=input_shape, name='img')\n\n    # Encoder\n    c1 = Conv2D(16, (3,3), padding='same')(inputs)\n    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n    c1 = Conv2D(16, (3,3), padding='same')(c1)\n    c1 = BatchNormalization()(c1); c1 = Activation('relu')(c1)\n    p1 = MaxPooling2D((2,2))(c1); p1 = Dropout(dropout_rate)(p1)   # 64 -> 32\n\n    c2 = Conv2D(32, (3,3), padding='same')(p1)\n    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n    c2 = Conv2D(32, (3,3), padding='same')(c2)\n    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n    p2 = MaxPooling2D((2,2))(c2); p2 = Dropout(dropout_rate)(p2)   # 32 -> 16\n\n    c3 = Conv2D(64, (3,3), padding='same')(p2)\n    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n    c3 = Conv2D(64, (3,3), padding='same')(c3)\n    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n    p3 = MaxPooling2D((2,2))(c3); p3 = Dropout(dropout_rate)(p3)   # 16 -> 8\n\n    c4 = Conv2D(128, (3,3), padding='same')(p3)\n    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n    c4 = Conv2D(128, (3,3), padding='same')(c4)\n    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n    p4 = MaxPooling2D((2,2))(c4); p4 = Dropout(dropout_rate)(p4)   # 8 -> 4\n\n    # Bottleneck (4x4)\n    c5 = Conv2D(256, (3,3), padding='same')(p4)\n    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n    c5 = Conv2D(256, (3,3), padding='same')(c5)\n    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n\n    # Decoder (back to 64x64)\n    u6 = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same')(c5)  # 4 -> 8\n    u6 = concatenate([u6, c4]); u6 = Dropout(dropout_rate)(u6)\n    u6 = Conv2D(128, (3,3), padding='same')(u6)\n    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n    u6 = Conv2D(128, (3,3), padding='same')(u6)\n    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n\n    u7 = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(u6)    # 8 -> 16\n    u7 = concatenate([u7, c3]); u7 = Dropout(dropout_rate)(u7)\n    u7 = Conv2D(64, (3,3), padding='same')(u7)\n    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n    u7 = Conv2D(64, (3,3), padding='same')(u7)\n    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n\n    u8 = Conv2DTranspose(32, (3,3), strides=(2,2), padding='same')(u7)    # 16 -> 32\n    u8 = concatenate([u8, c2]); u8 = Dropout(dropout_rate)(u8)\n    u8 = Conv2D(32, (3,3), padding='same')(u8)\n    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n    u8 = Conv2D(32, (3,3), padding='same')(u8)\n    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n\n    u9 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u8)    # 32 -> 64\n    u9 = concatenate([u9, c1]); u9 = Dropout(dropout_rate)(u9)\n    u9 = Conv2D(16, (3,3), padding='same')(u9)\n    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n    u9 = Conv2D(16, (3,3), padding='same')(u9)\n    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n\n    # Extra upsample to reach 128x128 (no skip)\n    u10 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u9)   # 64 -> 128\n    u10 = Dropout(dropout_rate)(u10)\n    u10 = Conv2D(16, (3,3), padding='same')(u10)\n    u10 = BatchNormalization()(u10); u10 = Activation('relu')(u10)\n\n    outputs = Conv2D(n_classes, (1,1), activation=final_activation, name='mask')(u10)\n    \n    return Model(inputs=inputs, outputs=outputs, name='UNet_64to128')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T11:31:17.365052Z","iopub.execute_input":"2025-09-11T11:31:17.365308Z","iopub.status.idle":"2025-09-11T11:31:17.389023Z","shell.execute_reply.started":"2025-09-11T11:31:17.365285Z","shell.execute_reply":"2025-09-11T11:31:17.388281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Model Initialization and Compilation\n\nThis cell initializes the U-Net model with the specified input shape and number of classes. It then compiles the model using the Adam optimizer and mean absolute error (MAE) as the loss function. The input and output shapes are printed to verify the model architecture.","metadata":{}},{"cell_type":"code","source":"model = unet_64to128(input_shape=(64,64,3), n_classes=3, final_activation='sigmoid')\n\nprint('Input shape:', model.input_shape)   # (None, 64, 64, 3)\nprint('Output shape:', model.output_shape) # (None, 128, 128, 3)\n\nmodel.compile(optimizer=Adam(1e-4), loss='mae' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T11:31:19.937456Z","iopub.execute_input":"2025-09-11T11:31:19.937798Z","iopub.status.idle":"2025-09-11T11:31:20.97465Z","shell.execute_reply.started":"2025-09-11T11:31:19.937768Z","shell.execute_reply":"2025-09-11T11:31:20.97378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Data Generator\n\nThis cell defines a data generator function `datagen` that loads and preprocesses images from the CelebA dataset. It resizes the images to 128x128 as ground truth and to 64x64 as low-resolution input, and normalizes the pixel values. The generator yields batches of low-resolution and high-resolution image pairs for training.","metadata":{}},{"cell_type":"code","source":"import os \nimport cv2 as cv\nimport numpy as np\n\nimgs = os.listdir('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/')\n\n\ndef datagen(batch_size):\n    \n    while True:\n        x_batch = []\n        y_batch = []\n        \n        for _ in range(batch_size):\n            indx = np.random.randint(0, len(imgs))\n            \n            bgr = cv.imread('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' + imgs[indx])\n            bgr = cv.resize(bgr, (128, 128))\n            rgb = cv.cvtColor(bgr, cv.COLOR_BGR2RGB)\n\n            # blur = cv.blur(rgb, (4, 4))\n\n            x = cv.resize(rgb, (64, 64))\n            x = x / 255.0\n            y = rgb / 255.0\n\n            x_batch.append(x)\n            y_batch.append(y)\n        \n        x_batch = np.array(x_batch).reshape(batch_size, 64, 64, 3)\n        y_batch = np.array(y_batch).reshape(batch_size, 128, 128, 3)\n        \n        yield x_batch, y_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T11:31:29.8844Z","iopub.execute_input":"2025-09-11T11:31:29.884734Z","iopub.status.idle":"2025-09-11T11:31:31.558665Z","shell.execute_reply.started":"2025-09-11T11:31:29.884705Z","shell.execute_reply":"2025-09-11T11:31:31.557905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Model Training (Short Run)\n\nThis cell trains the U-Net model for a small number of epochs (5) using the `datagen` function. This is a short run to quickly check if the model is training without errors.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\nbatch_size = 32\nnum_images = len(imgs)\nsteps_per_epoch = num_images // batch_size \n\n\nresults = model.fit(datagen(batch_size=batch_size), steps_per_epoch=steps_per_epoch , epochs=5, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T07:51:09.119191Z","iopub.execute_input":"2025-09-11T07:51:09.119497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Model Training with Callbacks and Sample Saving\n\nThis cell trains the U-Net model for a longer duration (3 epochs) and includes custom callbacks. The `show_and_save_samples` function generates and saves sample super-resolved images at the end of each epoch, along with the original, ground truth, and low-resolution images for comparison. The `LambdaCallback` is used to execute this function after each epoch. The model checkpoints are also saved during training.","metadata":{}},{"cell_type":"code","source":"import os, cv2 as cv, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n\ndef show_and_save_samples(model, imgs, base_dir, epoch, n=5, save_dir='./epoch_samples'):\n    os.makedirs(save_dir, exist_ok=True)\n    chosen = np.random.choice(imgs, n, replace=False)\n    fig, axes = plt.subplots(n, 4, figsize=(16, 4*n))\n    if n == 1: axes = np.expand_dims(axes, 0)\n\n    for i, fname in enumerate(chosen):\n        path = os.path.join(base_dir, fname)\n        rgb = cv.cvtColor(cv.imread(path), cv.COLOR_BGR2RGB)\n        gt128 = cv.resize(rgb, (128,128)).astype('float32')/255\n        img64 = cv.resize(rgb, (64,64)).astype('float32')/255\n        pred  = model.predict(np.expand_dims(img64,0), verbose=0)[0]\n        lowup = cv.resize((img64*255).astype(np.uint8),(128,128))\n\n        for ax, im, title in zip(axes[i],[rgb,gt128,lowup,pred],\n                                 ['Original','GT 128x128','Low-Res','Prediction']):\n            ax.imshow(im); ax.set_title(title); ax.axis('off')\n\n    plt.tight_layout(); plt.savefig(f\"{save_dir}/epoch_{epoch+1:02d}.png\"); plt.show()\n\n# --- Simple Callbacks ---\ndef on_epoch_end(epoch, logs):\n    show_and_save_samples(model, imgs, base_dir, epoch, n=5)\n    model.save(f'./checkpoints/model_epoch_{epoch+1:02d}.keras')\n\nshow_cb = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n\n\nbatch_size = 32\nsteps_per_epoch = len(imgs)//batch_size\nbase_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\nos.makedirs('./checkpoints', exist_ok=True)\n\nresults = model.fit(\n    datagen(batch_size=batch_size),\n    steps_per_epoch=steps_per_epoch,\n    epochs=3,\n    callbacks=[show_cb],\n    verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T11:31:33.315252Z","iopub.execute_input":"2025-09-11T11:31:33.315565Z","iopub.status.idle":"2025-09-11T12:19:03.662362Z","shell.execute_reply.started":"2025-09-11T11:31:33.315533Z","shell.execute_reply":"2025-09-11T12:19:03.661574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Project Summary\n\nThis notebook successfully implemented and trained a U-Net model for image super-resolution. The model was trained on the CelebA dataset, taking 64x64 images as input and generating 128x128 images.\n\nThe training process showed a decrease in the mean absolute error (MAE) loss over epochs, indicating that the model is learning to reconstruct higher-resolution images. The generated sample images at the end of each epoch provide a visual representation of the model's progress.\n\nFurther improvements could include:\n- Experimenting with different loss functions (e.g., perceptual loss)\n- Using a larger and more diverse dataset\n- Implementing more advanced super-resolution techniques (e.g., Generative Adversarial Networks)\n- Hyperparameter tuning to optimize model performance","metadata":{}}]}