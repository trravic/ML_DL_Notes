{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6WzMBQBlq5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment: Code-Focused Inference\n",
        "\n",
        "Your task is to load a pre-trained GPT-2 model and configure it to answer *only* questions related to Python coding.\n",
        "\n",
        "1. **Load Model and Tokenizer:** Load a suitable pre-trained GPT-2 model and its corresponding tokenizer. You can use `transformers.AutoModelForCausalLM` and `transformers.AutoTokenizer`. A smaller model like `gpt2` or `gpt2-medium` might be sufficient.\n",
        "2. **Implement a Filtering Mechanism:** Before generating a response, check if the input prompt is related to Python coding. You can use simple keyword matching (e.g., \"Python\", \"code\", \"function\", \"class\", \"import\") or a more sophisticated approach using a text classification model (optional).\n",
        "3. **Generate Response:** If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model.\n",
        "4. **Handle Non-Coding Questions:** If the prompt is not related to Python coding, return a predefined message indicating that the model can only answer coding questions.\n",
        "5. **Test:** Test your implementation with various prompts, including both Python coding questions and non-coding questions, to ensure the filtering mechanism works correctly."
      ],
      "metadata": {
        "id": "iOiqWaItl1fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Model and Tokenizer\n",
        "\n",
        " Load a suitable pre-trained GPT-2 model and its corresponding tokenizer. You can use `transformers.AutoModelForCausalLM` and `transformers.AutoTokenizer`. A smaller model like `gpt2` or `gpt2-medium` might be sufficient."
      ],
      "metadata": {
        "id": "mPGl7S49l3QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "930a614a61e047f9a352c9d42cefa026",
            "e3aa7ead3db043d5a250f274ab88d7c0",
            "a2ad1d90f0cc4417a0d221c880da4a7f",
            "3c4d891b1d2a452c869de45f09aeaac5",
            "23ffa04830ea465ab5ddd2bbd6c1b579",
            "c400173772744bfc88a7007b9f8f0fb5",
            "4dad14cabe914bb59a740dee49358dad",
            "e1de519f321a4a93a6fa4cc7810d0ca7",
            "c5c3ff4766d94e77ba149248eaccd8b0",
            "e15a2a6490f9438a8a47612a5f435c9d",
            "670df56decdf4657b7c9b13a532e8ec6",
            "0780cb75b97245ffafbf9cbf6a682537",
            "6e9e963cbd8c431faa8e57d22a9c2efe",
            "40be88f2fa754bbc8015549955dca6cf",
            "1f68ae29788a44a7850905344297a735",
            "80ee9e5b72b84b48bc33c718ea89194d",
            "c323176c683b4b6f8994aad6bc5d00c5",
            "23c0ac1b1ef44188b77b1e61f00559b2",
            "7b249e8dcb044d239b2d8e9bd506947e",
            "7381c25cfab6496e9b7a407a66a99350",
            "f52c4371ece341c184cf172f2b5092c8",
            "272299d44734453da264fcac973f69f1",
            "ea85d48d602e492ca5252d5dc7fad3ec",
            "565186d616044ffe8b3f439da2bb5d7b",
            "7424348c2abd4fd39b700b19c6219b4c",
            "e17007782c244e669e024ad06b67a92d",
            "3185c3bc5c96405fa30da2b43696c26b",
            "e4bf89d2e81340dbbc44fd926ea4121c",
            "2e52d81af0b1481d961f377711ce772b",
            "808ca853e6e04bfba1c4cdde99f4eca9",
            "75fb05abf3c2488db98279e05583dbea",
            "aee196034f4b44208ae9cd6f3de8bd9f",
            "e71ee662628749bb961611d194eb8b6b",
            "781a74896044417e8ce36b95205dade9",
            "e8e312abe1e6476b8a5770cd58c61410",
            "babb9d8a69824dc1ab9c5aa4369557a4",
            "fd073532a7e849bca39f14fc654b4c3d",
            "7f85a75b32824998b3828357cce314fb",
            "4ad5c08bee2749a9b6971d4380ee8335",
            "3f4f1ce3fe494b95960404db6772f75d",
            "bb724b9bc1884c4d91b3c8daff4fdc89",
            "cbae600f352a4436bbf16c73bfaa4439",
            "fffd5ce2f8bb4fbe9c7f84e18931c0ca",
            "df5646a5152041fe991e99f3e557862a",
            "207fe88c5c854c8aa301e247396b20b8",
            "4b157426378d4e9084acbb67639387ac",
            "4e225b4758ee418cac9ff523ce5b4335",
            "4001a8e6c65c46e2a2fad0427cc5b593",
            "7621a995a679484d8f5d524d0fddd544",
            "fffb6bfc5d39427fa05c1e5ee805987b",
            "a818405ff459427c892ac01cac6a3a33",
            "d23fc36fc8144b0fa8750a670f9e0558",
            "af23f277af46494ab0ec3c78356e6e64",
            "be563b0cbf8a48198e5367cc34d3926c",
            "800e504d67d744aa9115e0af0f08fc03",
            "db1590cea2eb4b26bf1acd527686461c",
            "d90ca2ba7d3647469a073b77a0569ef7",
            "4c8971837d10447e8e9a8ec3df300372",
            "f9464e334fb84716a2a33185aaa8d06e",
            "ef03ed8adb564b6fbaecfeac92315ff5",
            "f34555a16ac3456cb188ec56511641be",
            "761271ca8d61484ca042d8998a550d0f",
            "68f96a2fe99d49589e72d6875aa097ae",
            "0696afe95a5045bdb202bb45b36fe56d",
            "b737791c23194e8286ece94586c6437d",
            "47692b147bb04b0bbb489f3284b71b19",
            "ff121feed080426aa8c530b4f2c48871",
            "953ec3c75a194e59a7eecb07d8b7d64d",
            "7026b3f8989c4f04b5657bf53255cd67",
            "62e3eeb071a2467dba6ab5e610b634dc",
            "8ccb5ef783ad4988ac54204c1b1447bb",
            "e3a46344ce1f41608c428070c918d433",
            "6337602ba19544f29b8e591605c74e07",
            "fbeef89fbaa94a9a80280ef1811ae819",
            "425e8f1b002547e0aceee91bca4102f6",
            "a7a98c1c9b474600915303c8752c9a64",
            "3e2ea7e0cbe4463a85a44679a99b5990"
          ]
        },
        "id": "Nyp1lpJJl1yM",
        "outputId": "8e2a4844-a43a-4f39-9d56-ff997b0580da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "930a614a61e047f9a352c9d42cefa026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0780cb75b97245ffafbf9cbf6a682537"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea85d48d602e492ca5252d5dc7fad3ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "781a74896044417e8ce36b95205dade9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "207fe88c5c854c8aa301e247396b20b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db1590cea2eb4b26bf1acd527686461c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff121feed080426aa8c530b4f2c48871"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Implement a Filtering Mechanism:**\n",
        "\n",
        "Before generating a response, check if the input prompt is related to Python coding. You can use simple keyword matching (e.g., \"Python\", \"code\", \"function\", \"class\", \"import\") or a more sophisticated approach using a text classification model (optional)"
      ],
      "metadata": {
        "id": "BJLlacZMoPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_python_coding_question(prompt):\n",
        "    \"\"\"\n",
        "    Checks if the input prompt is related to Python coding using keyword matching.\n",
        "\n",
        "    Args:\n",
        "        prompt: The input string.\n",
        "\n",
        "    Returns:\n",
        "        True if the prompt contains Python coding keywords, False otherwise.\n",
        "    \"\"\"\n",
        "    python_keywords = [\n",
        "        \"python\", \"code\", \"function\", \"class\", \"import\", \"def\", \"print\",\n",
        "        \"loop\", \"list\", \"dictionary\", \"numpy\", \"pandas\", \"matplotlib\",\n",
        "        \"sklearn\", \"tensorflow\", \"pytorch\", \"def\", \"return\", \"if\", \"else\",\n",
        "        \"elif\", \"for\", \"while\", \"try\", \"except\", \"finally\", \"with\", \"as\"\n",
        "    ]\n",
        "    prompt_lower = prompt.lower()\n",
        "    for keyword in python_keywords:\n",
        "        if keyword in prompt_lower:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "pkM-ZMqvl-HW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Generate Response:** If the prompt is deemed a Python coding question, generate a response using the loaded GPT-2 model."
      ],
      "metadata": {
        "id": "GSR71t6YoZAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def answer_coding_question(prompt, model, tokenizer, max_len=100):\n",
        "    \"\"\"\n",
        "    Decides whether to generate a response using the GPT-2 model or return a\n",
        "    predefined message based on whether the prompt is a Python coding question.\n",
        "\n",
        "    Args:\n",
        "        prompt: The input string.\n",
        "        model: The pre-trained GPT-2 model.\n",
        "        tokenizer: The corresponding tokenizer.\n",
        "        max_len: The maximum length of the generated response.\n",
        "\n",
        "    Returns:\n",
        "        The generated response if the prompt is a coding question,\n",
        "        otherwise a predefined message.\n",
        "    \"\"\"\n",
        "    if is_python_coding_question(prompt):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "\n",
        "        # Ensure pad token is set before tokenization\n",
        "        if tokenizer.pad_token is None:\n",
        "             tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_len,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "    else:\n",
        "        return \"I can only answer questions related to Python coding.\""
      ],
      "metadata": {
        "id": "oG0XkKMpoUkH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validations on python - non-python coding questions:\n",
        "\n"
      ],
      "metadata": {
        "id": "9SyCST31o1dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Call with a Python coding question\n",
        "prompt_coding_1 = \"How to define a function in Python?\"\n",
        "response_coding_1 = answer_coding_question(prompt_coding_1, model, tokenizer)\n",
        "print(f\"Prompt: {prompt_coding_1}\")\n",
        "print(f\"Response: {response_coding_1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNa5y-BAo3LU",
        "outputId": "308f5be4-b547-4691-a591-e9cafe36015a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: How to define a function in Python?\n",
            "Response: How to define a function in Python?\n",
            "\n",
            "The following code snippet shows how to define a function in Python.\n",
            "\n",
            "import sys import time import sys.argv import sys.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.argv.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Call with a non-coding question\n",
        "prompt_non_coding_1 = \"What is the capital of France?\"\n",
        "response_non_coding_1 = answer_coding_question(prompt_non_coding_1, model, tokenizer)\n",
        "print(f\"Prompt: {prompt_non_coding_1}\")\n",
        "print(f\"Response: {response_non_coding_1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx48Lfz3o4ep",
        "outputId": "f390c955-b9ca-45e0-abea-c1b9288f2788"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is the capital of France?\n",
            "Response: I can only answer questions related to Python coding.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Call with another Python coding question\n",
        "prompt_coding_2 = \"Explain Python lists.\"\n",
        "response_coding_2 = answer_coding_question(prompt_coding_2, model, tokenizer)\n",
        "print(f\"Prompt: {prompt_coding_2}\")\n",
        "print(f\"Response: {response_coding_2}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnjLc6Dno8qF",
        "outputId": "da5849ea-bf66-41a8-92ee-b5130bb5b6a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Explain Python lists.\n",
            "Response: Explain Python lists.\n",
            "\n",
            "The list is a list of Python objects that are used to represent the Python objects in the list.\n",
            "\n",
            "The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used to represent the Python objects in the list. The list is a list of Python objects that are used\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Call with another non-coding question\n",
        "prompt_non_coding_2 = \"Tell me a joke.\"\n",
        "response_non_coding_2 = answer_coding_question(prompt_non_coding_2, model, tokenizer)\n",
        "print(f\"Prompt: {prompt_non_coding_2}\")\n",
        "print(f\"Response: {response_non_coding_2}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnOsZK9lo-hz",
        "outputId": "95a791cb-3fd6-4dcd-b47d-98e0f43ea30f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Tell me a joke.\n",
            "Response: I can only answer questions related to Python coding.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0u2LuSQLpAqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}